{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "771677a7-74dc-4b3c-86ab-930dd1be7fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. One-hot vector for 'cat':\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "2. Bag-of-Words representation (doc x vocab):\n",
      "[[0 0 0 0 1 0 0 0 0 1 1 1 2]\n",
      " [0 0 1 1 1 0 1 0 0 0 0 0 2]\n",
      " [1 1 0 0 0 1 0 1 1 0 0 0 0]]\n",
      "Vocabulary: ['and' 'are' 'at' 'barked' 'cat' 'cats' 'dog' 'dogs' 'friends' 'mat' 'on'\n",
      " 'sat' 'the']\n",
      "\n",
      "3. TF-IDF representation:\n",
      "[[0.         0.         0.         0.         0.31331607 0.\n",
      "  0.         0.         0.         0.41197298 0.41197298 0.41197298\n",
      "  0.62663214]\n",
      " [0.         0.         0.41197298 0.41197298 0.31331607 0.\n",
      "  0.41197298 0.         0.         0.         0.         0.\n",
      "  0.62663214]\n",
      " [0.4472136  0.4472136  0.         0.         0.         0.4472136\n",
      "  0.         0.4472136  0.4472136  0.         0.         0.\n",
      "  0.        ]]\n",
      "Vocabulary: ['and' 'are' 'at' 'barked' 'cat' 'cats' 'dog' 'dogs' 'friends' 'mat' 'on'\n",
      " 'sat' 'the']\n",
      "\n",
      "4. Word2Vec embedding for 'cat':\n",
      "[-0.01631583  0.0089916  -0.00827415  0.00164907  0.01699724 -0.00892435\n",
      "  0.009035   -0.01357392 -0.00709698  0.01879702]\n",
      "\n",
      "5. GloVe embedding for 'cat':\n",
      "[ 0.45281  -0.50108  -0.53714  -0.015697  0.22191   0.54602  -0.67301\n",
      " -0.6891    0.63493  -0.19726 ]\n",
      "\n",
      "6. FastText embedding for 'cats':\n",
      "[-2.3170598e-03 -2.0539537e-03  1.0790244e-03 -6.1491965e-03\n",
      "  5.3702937e-03 -2.6794572e-04 -7.4717490e-04 -2.7384354e-05\n",
      "  2.9085550e-04  2.4583337e-03]\n",
      "\n",
      "8. BERT embedding shape: torch.Size([1, 8, 768])\n",
      "WARNING:tensorflow:From C:\\Users\\nvqua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nvqua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nvqua\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9. Sentence-BERT embedding (dim): (384,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nvqua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nvqua\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10. GPT2 embedding shape: torch.Size([1, 6, 768])\n"
     ]
    }
   ],
   "source": [
    "# === Text Representation Techniques Demo ===\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog barked at the cat\",\n",
    "    \"Dogs and cats are friends\"\n",
    "]\n",
    "\n",
    "# 1. One-Hot Encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "vocab = list(set(\" \".join(corpus).lower().split()))\n",
    "onehot = OneHotEncoder(sparse_output=False)\n",
    "onehot.fit(np.array(vocab).reshape(-1,1))\n",
    "\n",
    "print(\"1. One-hot vector for 'cat':\")\n",
    "print(onehot.transform([[\"cat\"]]))\n",
    "\n",
    "\n",
    "# 2. Bag-of-Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer()\n",
    "X_bow = bow.fit_transform(corpus)\n",
    "print(\"\\n2. Bag-of-Words representation (doc x vocab):\")\n",
    "print(X_bow.toarray())\n",
    "print(\"Vocabulary:\", bow.get_feature_names_out())\n",
    "\n",
    "\n",
    "# 3. TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(corpus)\n",
    "print(\"\\n3. TF-IDF representation:\")\n",
    "print(X_tfidf.toarray())\n",
    "print(\"Vocabulary:\", tfidf.get_feature_names_out())\n",
    "\n",
    "\n",
    "# 4. Word2Vec (using gensim)\n",
    "from gensim.models import Word2Vec\n",
    "sentences = [doc.lower().split() for doc in corpus]\n",
    "w2v = Word2Vec(sentences, vector_size=50, window=3, min_count=1, sg=1)\n",
    "print(\"\\n4. Word2Vec embedding for 'cat':\")\n",
    "print(w2v.wv['cat'][:10])  # show first 10 dims\n",
    "\n",
    "\n",
    "# 5. GloVe (via gensim downloader)\n",
    "import gensim.downloader as api\n",
    "glove = api.load(\"glove-wiki-gigaword-50\")\n",
    "print(\"\\n5. GloVe embedding for 'cat':\")\n",
    "print(glove['cat'][:10])\n",
    "\n",
    "\n",
    "# 6. FastText\n",
    "from gensim.models.fasttext import FastText\n",
    "ft = FastText(sentences, vector_size=50, window=3, min_count=1)\n",
    "print(\"\\n6. FastText embedding for 'cats':\")\n",
    "print(ft.wv['cats'][:10])  # works for plural form\n",
    "\n",
    "\n",
    "# 7. ELMo (via allennlp)\n",
    "# from allennlp.commands.elmo import ElmoEmbedder\n",
    "# elmo = ElmoEmbedder()\n",
    "# tokens = [\"the\", \"cat\", \"sat\"]\n",
    "# embeddings = elmo.embed_sentence(tokens)\n",
    "# print(\"\\n7. ELMo embedding for 'cat' (dim):\", embeddings.shape)\n",
    "\n",
    "\n",
    "# 8. BERT\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = tokenizer(\"The cat sat on the mat\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(\"\\n8. BERT embedding shape:\", outputs.last_hidden_state.shape)\n",
    "\n",
    "\n",
    "# 9. Sentence-BERT (sentence embeddings)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "sent_emb = sbert.encode(\"The cat sat on the mat\")\n",
    "print(\"\\n9. Sentence-BERT embedding (dim):\", sent_emb.shape)\n",
    "\n",
    "\n",
    "# 10. GPT-like embeddings (OpenAI or HuggingFace models)\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "mdl = AutoModel.from_pretrained(\"gpt2\")\n",
    "inp = tok(\"The cat sat on the mat\", return_tensors=\"pt\")\n",
    "out = mdl(**inp)\n",
    "print(\"\\n10. GPT2 embedding shape:\", out.last_hidden_state.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4450b3a4-fc2a-486b-8aa3-f4bcf95f6f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
